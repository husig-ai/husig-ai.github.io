<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper-Exploration on Husig.ai</title>
    <link>http://localhost:1313/categories/paper-exploration/</link>
    <description>Recent content in Paper-Exploration on Husig.ai</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Sep 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/paper-exploration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Paper Exploration] A Unified Approach to Interpreting Model Predictions</title>
      <link>http://localhost:1313/posts/shap_exploration/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/shap_exploration/</guid>
      <description>[Paper Exploration] A Unified Approach to Interpreting Model Predictions Paper Authors: Scott M. Lundberg, Su-In Lee&#xA;Code: https://github.com/shap/shap&#xA;Original Paper:&#xA;Exploration Problem Understanding model predictions is crucial for many applications. However, complex models, like ensemble or deep learning models, (while they usually achieve high accuracy) are generally difficult to interpret. Existing interpretation methods lack clarity about their relationships and preferences. Proposed Solution by Authors Introduce a unified framework called SHAP (SHapley Additive exPlanations) for interpreting predictions.</description>
    </item>
  </channel>
</rss>
