<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper-Exploration on Husig.ai</title>
    <link>https://husig.ai/categories/paper-exploration/</link>
    <description>Recent content in Paper-Exploration on Husig.ai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 07 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://husig.ai/categories/paper-exploration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Paper Exploration] Deep Residual Learning for Image Recognition</title>
      <link>https://husig.ai/posts/resnets/</link>
      <pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/resnets/</guid>
      <description>[Paper Exploration] Deep Residual Learning for Image Recognition Author: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun&#xA;Published on 2015&#xA;Abstract Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.</description>
    </item>
    <item>
      <title>[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)</title>
      <link>https://husig.ai/posts/segment_anything/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/segment_anything/</guid>
      <description>[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM) Authors: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll√°r, Ross Girshick&#xA;Published on 2023&#xA;The Segment Anything Model (SAM) was developed by Meta AI as a foundation model for image segmentation tasks. The goal of SAM is to create a universal model that can efficiently handle various segmentation tasks with minimal prompting.</description>
    </item>
    <item>
      <title>[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification</title>
      <link>https://husig.ai/posts/zero-shot-classification/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/zero-shot-classification/</guid>
      <description>[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification Author: Pushpankar Kumar Pushp, Muktabh Mayank Srivastava&#xA;Published on 2017&#xA;Abstract Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence&amp;rsquo;s tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space.</description>
    </item>
    <item>
      <title>[Paper Exploration] Statistical Modeling: The Two Cultures</title>
      <link>https://husig.ai/posts/statistical_modelling_two_cultures/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/statistical_modelling_two_cultures/</guid>
      <description>[Paper Exploration] Statistical Modeling: The Two Cultures Abstract There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.</description>
    </item>
    <item>
      <title>[Paper Exploration] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://husig.ai/posts/transformers_for_image_paper_exploration/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/transformers_for_image_paper_exploration/</guid>
      <description>[Paper Exploration] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&amp;quot; Authors: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby&#xA;Published as a conference paper at ICLR 2021&#xA;Abstract While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
  </channel>
</rss>
