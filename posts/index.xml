<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Husig.ai</title>
    <link>https://husig.ai/posts/</link>
    <description>Recent content in Posts on Husig.ai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://husig.ai/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How we built a data obfuscation platform that made production data shareable</title>
      <link>https://husig.ai/posts/incog/</link>
      <pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/incog/</guid>
      <description>How intelligent data obfuscation unlocked collaboration without compromising privacy&#xA;Companies sit on valuable production data they can&amp;rsquo;t share—with vendors, partners, or even internal teams. PII regulations, contractual obligations, and security policies lock data behind approval processes that take weeks.&#xA;A healthcare company can&amp;rsquo;t share patient data with an ML vendor. A bank can&amp;rsquo;t give transaction logs to a fraud detection partner. An e-commerce platform can&amp;rsquo;t export customer behavior to their analytics consultants.</description>
    </item>
    <item>
      <title>How we built a synthetic data platform that cut ML development time by 60%</title>
      <link>https://husig.ai/posts/fabri/</link>
      <pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/fabri/</guid>
      <description>How synthetic data generation transformed ML development workflows&#xA;Machine learning teams waste weeks waiting for production data access. Legal reviews, anonymization pipelines, compliance approvals—by the time data arrives, project timelines have slipped. Development environments sit empty while engineers context-switch to other work.&#xA;We saw this problem across multiple client engagements and decided to build a solution: Fabri, a synthetic data generation platform that gives ML teams realistic, privacy-safe datasets in minutes instead of weeks.</description>
    </item>
    <item>
      <title>How we built a referral system without touching a single line of client code</title>
      <link>https://husig.ai/posts/referral_system/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/referral_system/</guid>
      <description>How we created a platform-agnostic referral tracking system that increased organizer acquisition by 40%&#xA;A ticketing startup came to us with a growth challenge: they wanted to launch a referral program to acquire new event organizers, but their engineering team was already stretched thin. They couldn&amp;rsquo;t afford to build referral infrastructure into their core platform.&#xA;Most referral solutions—Rewardful, FirstPromoter, PartnerStack—require deep integration with your billing and user systems. That wasn&amp;rsquo;t an option here.</description>
    </item>
    <item>
      <title>How we built an AI-Powered newsletter that increased ticket sales by 134%</title>
      <link>https://husig.ai/posts/ai_newsletter/</link>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/ai_newsletter/</guid>
      <description>A case study in turning event data into personalized engagement&#xA;A ticketing startup approached us with a familiar problem: they had thousands of users across dozens of cities, but their email marketing felt like shouting into the void. Generic blast emails. Low open rates. Even lower conversions.&#xA;They needed a system that could understand their users—where they lived, what events they attended, what they might want next—and reach them with the right message at the right time.</description>
    </item>
    <item>
      <title>What Actually Matters in AI for 2026: A Reality Check for Businesses</title>
      <link>https://husig.ai/posts/ai-business-trends-2026-reality-check/</link>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/ai-business-trends-2026-reality-check/</guid>
      <description>At Husig.ai, we spend our days in the trenches of digital transformation—navigating messy spreadsheets, legacy databases, and the manual processes that keep businesses running. We’re often asked if we’re building the next &amp;ldquo;world-changing&amp;rdquo; super-intelligence.&#xA;Our answer? No. We’re building tools that help you finish your work by 5 PM.&#xA;However, to build for today, we have to keep an eye on where the giants are heading. We recently analyzed the 2026 outlooks from Microsoft and Google Cloud.</description>
    </item>
    <item>
      <title>[Paper Exploration] Adam: A Method for Stochastic Optimization</title>
      <link>https://husig.ai/posts/adam/</link>
      <pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/adam/</guid>
      <description>Author: Diederik P. Kingma, Jimmy Ba&#xA;Published on 2014&#xA;Abstract We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.</description>
    </item>
    <item>
      <title>[Paper Exploration] Deep Residual Learning for Image Recognition</title>
      <link>https://husig.ai/posts/resnets/</link>
      <pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/resnets/</guid>
      <description>Author: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun&#xA;Published on 2015&#xA;Abstract Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.</description>
    </item>
    <item>
      <title>[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)</title>
      <link>https://husig.ai/posts/segment_anything/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/segment_anything/</guid>
      <description>Authors: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick&#xA;Published on 2023&#xA;The Segment Anything Model (SAM) was developed by Meta AI as a foundation model for image segmentation tasks. The goal of SAM is to create a universal model that can efficiently handle various segmentation tasks with minimal prompting.&#xA;Abstract We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation.</description>
    </item>
    <item>
      <title>[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification</title>
      <link>https://husig.ai/posts/zero-shot-classification/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/zero-shot-classification/</guid>
      <description>Author: Pushpankar Kumar Pushp, Muktabh Mayank Srivastava&#xA;Published on 2017&#xA;Abstract Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence&amp;rsquo;s tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space.</description>
    </item>
    <item>
      <title>[Paper Exploration] Statistical Modeling: The Two Cultures</title>
      <link>https://husig.ai/posts/statistical_modelling_two_cultures/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/statistical_modelling_two_cultures/</guid>
      <description>Abstract There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.</description>
    </item>
    <item>
      <title>[Paper Exploration] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://husig.ai/posts/transformers_for_image_paper_exploration/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/transformers_for_image_paper_exploration/</guid>
      <description>Authors: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby&#xA;Published as a conference paper at ICLR 2021&#xA;Abstract While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
  </channel>
</rss>
