<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on Husig.ai</title>
    <link>http://localhost:1313/tags/ml/</link>
    <description>Recent content in Ml on Husig.ai</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Paper Exploration] Statistical Modeling: The Two Cultures</title>
      <link>http://localhost:1313/posts/statistical_modelling_two_cultures/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/statistical_modelling_two_cultures/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;There are &lt;strong&gt;two cultures in the use of statistical modeling&lt;/strong&gt; to reach conclusions from data. One assumes that the data are generated by a given &lt;strong&gt;stochastic data model&lt;/strong&gt;. The other uses &lt;strong&gt;algorithmic models&lt;/strong&gt; and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. &lt;strong&gt;If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Paper Exploration] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>http://localhost:1313/posts/transformers_for_image_paper_exploration/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/transformers_for_image_paper_exploration/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Authors: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Published as a conference paper at ICLR 2021&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;While the &lt;strong&gt;Transformer architecture&lt;/strong&gt; has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, &lt;strong&gt;attention&lt;/strong&gt; is either applied in conjunction with &lt;strong&gt;convolutional networks&lt;/strong&gt;, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image &lt;strong&gt;patches&lt;/strong&gt; can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (&lt;strong&gt;ImageNet&lt;/strong&gt;, CIFAR-100, VTAB, etc.), &lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt; attains excellent results compared to &lt;strong&gt;state-of-the-art&lt;/strong&gt; convolutional networks while requiring substantially fewer computational resources to train.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
