<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zero-Shot on Husig.ai</title>
    <link>http://localhost:1313/tags/zero-shot/</link>
    <description>Recent content in Zero-Shot on Husig.ai</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/zero-shot/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)</title>
      <link>http://localhost:1313/posts/segment_anything/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/segment_anything/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Authors: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll√°r, Ross Girshick&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Published on 2023&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;The &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; was developed by Meta AI as a foundation model for image segmentation tasks. The goal of SAM is to create a universal model that can efficiently handle various segmentation tasks with minimal &lt;strong&gt;prompting&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://uploads-ssl.webflow.com/641bc1f4cda8707686f07277/643d06676d8d2025ce33b806_ezgif.com-video-to-gif-2.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive &amp;ndash; often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification</title>
      <link>http://localhost:1313/posts/zero-shot-classification/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/zero-shot-classification/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Author: Pushpankar Kumar Pushp, Muktabh Mayank Srivastava&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Published on 2017&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence&amp;rsquo;s tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space. The model learns to predict whether a given sentence is related to a tag or not; unlike other classifiers that learn to classify the sentence as one of the possible classes. We propose three different neural networks for the task and report their accuracy on the test set of the dataset used for training them as well as two other standard datasets for which no retraining was done. We show that our models generalize well across new unseen classes in both cases. Although the models do not achieve the accuracy level of the state of the art supervised models, yet it evidently is a step forward towards general intelligence in natural language processing.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
