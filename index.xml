<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Husig.ai</title>
    <link>https://husig.ai/</link>
    <description>Recent content in Posts on Husig.ai</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://husig.ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Engineer Associate</title>
      <link>https://husig.ai/careers/ai_engineer_associate_2_dec_25/</link>
      <pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/careers/ai_engineer_associate_2_dec_25/</guid>
      <description>We are seeking a talented AI Engineer Associate to join our engineering team and help build innovative AI-powered solutions, with a primary focus on developing a Real Estate AI-based CRM and other HuSig products. This role is perfect for someone who has strong coding fundamentals, understands data structures and algorithms, and is excited about working with modern AI technologies and full-stack development.&#xA;You&amp;rsquo;ll be contributing to production systems that serve real customers, and gaining hands-on experience with cutting-edge AI technologies including large language models, voice AI, and intelligent automation systems.</description>
    </item>
    <item>
      <title>[Paper Exploration] Adam: A Method for Stochastic Optimization</title>
      <link>https://husig.ai/posts/adam/</link>
      <pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/adam/</guid>
      <description>Author: Diederik P. Kingma, Jimmy Ba&#xA;Published on 2014&#xA;Abstract We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.</description>
    </item>
    <item>
      <title>[Paper Exploration] Deep Residual Learning for Image Recognition</title>
      <link>https://husig.ai/posts/resnets/</link>
      <pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/resnets/</guid>
      <description>Author: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun&#xA;Published on 2015&#xA;Abstract Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.</description>
    </item>
    <item>
      <title>[Paper Exploration] In-Depth Analysis of the Segment Anything Model (SAM)</title>
      <link>https://husig.ai/posts/segment_anything/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/segment_anything/</guid>
      <description>Authors: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll√°r, Ross Girshick&#xA;Published on 2023&#xA;The Segment Anything Model (SAM) was developed by Meta AI as a foundation model for image segmentation tasks. The goal of SAM is to create a universal model that can efficiently handle various segmentation tasks with minimal prompting.&#xA;Abstract We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation.</description>
    </item>
    <item>
      <title>[Paper Exploration] Train Once, Test Anywhere: Zero-Shot Learning for Text Classification</title>
      <link>https://husig.ai/posts/zero-shot-classification/</link>
      <pubDate>Thu, 20 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/zero-shot-classification/</guid>
      <description>Author: Pushpankar Kumar Pushp, Muktabh Mayank Srivastava&#xA;Published on 2017&#xA;Abstract Zero-shot Learners are models capable of predicting unseen classes. In this work, we propose a Zero-shot Learning approach for text categorization. Our method involves training model on a large corpus of sentences to learn the relationship between a sentence and embedding of sentence&amp;rsquo;s tags. Learning such relationship makes the model generalize to unseen sentences, tags, and even new datasets provided they can be put into same embedding space.</description>
    </item>
    <item>
      <title>[Paper Exploration] Statistical Modeling: The Two Cultures</title>
      <link>https://husig.ai/posts/statistical_modelling_two_cultures/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/statistical_modelling_two_cultures/</guid>
      <description>Abstract There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.</description>
    </item>
    <item>
      <title>[Paper Exploration] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://husig.ai/posts/transformers_for_image_paper_exploration/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://husig.ai/posts/transformers_for_image_paper_exploration/</guid>
      <description>Authors: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby&#xA;Published as a conference paper at ICLR 2021&#xA;Abstract While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.</description>
    </item>
  </channel>
</rss>
